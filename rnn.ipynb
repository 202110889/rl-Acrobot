{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    import torch_directml\n",
    "    global device\n",
    "    device = torch_directml.device()\n",
    "except (NameError, ModuleNotFoundError):\n",
    "    import intel_npu_acceleration_library\n",
    "    from intel_npu_acceleration_library import compile\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "except (NameError, ModuleNotFoundError):\n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='Acrobot-v1', entry_point='gymnasium.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='Acrobot', version=1, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym  # As a best practice, Gymnasium is usually imported as 'gym'\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "print(env.spec)\n",
    "\n",
    "plt.ion() # matplotlib를 interactive mode로 설정 -> 그래프를 실시간으로 업데이트할 수 있도록 함\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    \n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Reward')\n",
    "    \n",
    "    # 100 에피소드 이동 평균 계산 및 플롯\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100 Episode Avg', linestyle='--')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingRQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, n_sequence):\n",
    "        super(DuelingRQN, self).__init__()\n",
    "        # input: (N, seq, 4), output: (output(N, seq, h_out), hiddenstate(layers, 32, h_out)). seq는 일단 1로 \n",
    "        # hidden state: (layers, seq, hidden_size)\n",
    "        self.rnn = nn.RNN(n_observations, hidden_size=48, num_layers=3, batch_first=True) \n",
    "        # DuelingDQN 적용\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_value = nn.Linear(48, 1)\n",
    "        self.fc_advantage = nn.Linear(48, n_actions)\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x) # (BATCH, seq, 24), (3, BATCH, 24)\n",
    "        out = self.flatten(F.tanh(out))\n",
    "        # 상태값과 행동 이점 계산\n",
    "        value = self.fc_value(out)\n",
    "        advantage = self.fc_advantage(out)\n",
    "\n",
    "        # 최종 Q값 계산\n",
    "        q_values = value + advantage\n",
    "        return q_values\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a481ad0b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 파라미터\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 10000 # 크게 할수록 캄험이 더 오래 지속 -> 새로운 더 많이 탐색\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "SEED = 42\n",
    "\n",
    "n_actions = env.action_space.n # 3\n",
    "\n",
    "n_observations = env.observation_space._shape[0]\n",
    "n_sequence = 16\n",
    "\n",
    "# 입력 텐서 형식은 (배치 크기, 채널 수, 높이, 너비)\n",
    "policy_net = DuelingRQN(n_observations, n_actions, n_sequence).to(device)\n",
    "target_net = DuelingRQN(n_observations, n_actions, n_sequence).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "train_data: list[Transition] = []\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 7200\n",
    "else:\n",
    "    num_episodes = 600\n",
    "\n",
    "env.action_space.seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(train_data, loss_fn, optimizer):\n",
    "    batch = Transition(*zip(*train_data)) \n",
    "    # 다음 상태 중 None이 아닌 것만 선택\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states = torch.stack(\n",
    "        [s for s in batch.next_state if s is not None]\n",
    "    ).to(device)\n",
    "    # 모든 상태를 (배치 크기, 채널 수) 형식으로 연결\n",
    "    state_batch = torch.cat(batch.state).to(device)  # (BATCH_SIZE, 6)\n",
    "    action_batch = torch.cat(batch.action).to(device) # (BATCH_SIZE, )\n",
    "    reward_batch = torch.cat(batch.reward).to(device)  # 배치 처리를 최대한 병렬로 처리\n",
    "\n",
    "    # Q값 계산\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 다음 상태의 Q값 계산\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Double DQN으로 수정\n",
    "        next_actions = policy_net(non_final_next_states).max(1).indices.unsqueeze(1)\n",
    "        next_state_values[non_final_mask] = (\n",
    "            target_net(non_final_next_states).gather(1, next_actions).squeeze(1)\n",
    "        )\n",
    "    # 기대 Q값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 함수와 중요도 가중치 적용\n",
    "    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))  # reduction=\"none\"\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(\n",
    "        -1.0 * steps_done / EPS_DECAY\n",
    "    )\n",
    "    steps_done += 1\n",
    "\n",
    "    state = state.to(device)\n",
    "\n",
    "    # 초기 eps_threshold는 1.0이다. steps_done이 증가함에 따라 점진적으로 감소하여 결국 Q-value가 가장 큰 행동을 선택.\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "\n",
    "    else:\n",
    "        return torch.tensor(\n",
    "            [[env.action_space.sample()]], device=device, dtype=torch.long\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards, path=\"models\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    save_path = os.path.join(path, f\"policy_net_episode_{episode}.pth\")\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model_state_dict': policy_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'steps_done': steps_done,  # 탐험 상태 저장\n",
    "        'episode_rewards': episode_rewards\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def load_model(policy_net, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    # 모델 가중치와 옵티마이저 복원\n",
    "    policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # 에피소드 및 탐험상태 복원\n",
    "    episode = checkpoint['episode']\n",
    "    steps_done = checkpoint.get('steps_done', 0)  # 기본값 0\n",
    "    \n",
    "    # 에피소드별 보상 복원\n",
    "    episode_rewards = checkpoint.get('episode_rewards', [])\n",
    "\n",
    "    print(f\"Model loaded from {path}, starting from episode {episode}, steps_done={steps_done}\")\n",
    "    return episode, steps_done, episode_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델에서 학습 재개\n",
    "# checkpoint_path = \"models/policy_net_episode_4.pth\"\n",
    "# start_episode, steps_done, episode_rewards = load_model(policy_net, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "    memory = deque(maxlen=BATCH_SIZE)\n",
    "    for episode in range(num_episodes+1):\n",
    "        state, info = env.reset(seed=42)\n",
    "        state = torch.tensor(state, device=device).unsqueeze(0)\n",
    "\n",
    "        episode_reward = 0\n",
    "        action_counts = np.zeros(env.action_space.n)\n",
    "        train_data = []\n",
    "\n",
    "        for t in count(): # 무한 반복\n",
    "            if episode < 10:  # 초기 10 에피소드는 무조건 랜덤 행동\n",
    "                action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "            else:\n",
    "                action = select_action(state)\n",
    "\n",
    "            action_counts[action] += 1\n",
    "            # 행동을 선택하면, 해당 행동이 4개의 연속된 프레임 동안 환경에 반복적으로 적용\n",
    "            # 프레임 스킵 동안 발생한 보상이 모두 합산되어 반환, 마지막 프레임의 상태와 종료 여부가 반환\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            episode_reward += reward.item()\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if not done:\n",
    "                next_state = torch.tensor(next_state, device=device).unsqueeze(0)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            memory.append((state, action, next_state, reward))\n",
    "            state = next_state\n",
    "\n",
    "            # policy network 학습\n",
    "            if t != 0 and t % BATCH_SIZE == 0:\n",
    "                optimize_model(memory, loss_fn=loss_fn, optimizer=optimizer)\n",
    "\n",
    "            if t % (BATCH_SIZE * 4) == 0:\n",
    "                # 소프트 업데이트 적용 (target network 업데이트) -> 16 프레임마다 학습이 이루어짐\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            # 행동 비율 출력\n",
    "            action_distribution = action_counts / action_counts.sum()\n",
    "            print(f\"Action distribution: {action_distribution}\")            \n",
    "            plot_rewards()\n",
    "\n",
    "    print('Complete')\n",
    "    plot_rewards(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving the current model...\")\n",
    "    save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards)\n",
    "    env.close()\n",
    "    print(\"Model saved. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -415.0,\n",
       " -409.0,\n",
       " -495.0,\n",
       " -500.0,\n",
       " -397.0,\n",
       " -500.0,\n",
       " -452.0,\n",
       " -286.0,\n",
       " -488.0,\n",
       " -500.0,\n",
       " -324.0,\n",
       " -447.0,\n",
       " -353.0,\n",
       " -426.0,\n",
       " -465.0,\n",
       " -235.0,\n",
       " -487.0,\n",
       " -316.0,\n",
       " -331.0,\n",
       " -500.0,\n",
       " -165.0,\n",
       " -277.0,\n",
       " -283.0,\n",
       " -199.0,\n",
       " -267.0,\n",
       " -238.0,\n",
       " -161.0,\n",
       " -416.0,\n",
       " -449.0,\n",
       " -258.0,\n",
       " -245.0,\n",
       " -289.0,\n",
       " -270.0,\n",
       " -195.0,\n",
       " -224.0,\n",
       " -184.0,\n",
       " -383.0,\n",
       " -222.0,\n",
       " -231.0,\n",
       " -250.0,\n",
       " -155.0,\n",
       " -155.0,\n",
       " -203.0,\n",
       " -165.0,\n",
       " -185.0,\n",
       " -132.0,\n",
       " -226.0,\n",
       " -154.0,\n",
       " -189.0,\n",
       " -109.0,\n",
       " -176.0,\n",
       " -118.0,\n",
       " -109.0,\n",
       " -179.0,\n",
       " -101.0,\n",
       " -107.0,\n",
       " -201.0,\n",
       " -151.0,\n",
       " -109.0,\n",
       " -160.0,\n",
       " -260.0,\n",
       " -169.0,\n",
       " -191.0,\n",
       " -143.0,\n",
       " -167.0,\n",
       " -154.0,\n",
       " -146.0,\n",
       " -123.0,\n",
       " -234.0,\n",
       " -183.0,\n",
       " -138.0,\n",
       " -139.0,\n",
       " -132.0,\n",
       " -160.0,\n",
       " -182.0,\n",
       " -180.0,\n",
       " -162.0,\n",
       " -150.0,\n",
       " -116.0,\n",
       " -126.0,\n",
       " -112.0,\n",
       " -150.0,\n",
       " -161.0,\n",
       " -106.0,\n",
       " -101.0,\n",
       " -103.0,\n",
       " -132.0,\n",
       " -155.0,\n",
       " -165.0,\n",
       " -124.0,\n",
       " -149.0,\n",
       " -125.0,\n",
       " -156.0,\n",
       " -151.0,\n",
       " -121.0,\n",
       " -120.0,\n",
       " -167.0,\n",
       " -101.0,\n",
       " -152.0,\n",
       " -144.0,\n",
       " -133.0,\n",
       " -108.0,\n",
       " -132.0,\n",
       " -128.0,\n",
       " -111.0,\n",
       " -219.0,\n",
       " -110.0,\n",
       " -127.0,\n",
       " -136.0,\n",
       " -130.0,\n",
       " -108.0,\n",
       " -104.0,\n",
       " -153.0,\n",
       " -122.0,\n",
       " -144.0,\n",
       " -145.0,\n",
       " -151.0,\n",
       " -107.0,\n",
       " -162.0,\n",
       " -133.0,\n",
       " -135.0,\n",
       " -101.0,\n",
       " -112.0,\n",
       " -163.0,\n",
       " -163.0,\n",
       " -240.0,\n",
       " -238.0,\n",
       " -157.0,\n",
       " -140.0,\n",
       " -139.0,\n",
       " -146.0,\n",
       " -352.0,\n",
       " -127.0,\n",
       " -168.0,\n",
       " -98.0,\n",
       " -179.0,\n",
       " -135.0,\n",
       " -146.0,\n",
       " -200.0,\n",
       " -178.0,\n",
       " -112.0,\n",
       " -96.0,\n",
       " -145.0,\n",
       " -135.0,\n",
       " -139.0,\n",
       " -140.0,\n",
       " -118.0,\n",
       " -111.0,\n",
       " -150.0,\n",
       " -163.0,\n",
       " -158.0,\n",
       " -128.0,\n",
       " -148.0,\n",
       " -122.0,\n",
       " -259.0,\n",
       " -138.0,\n",
       " -147.0,\n",
       " -115.0,\n",
       " -148.0,\n",
       " -124.0,\n",
       " -132.0,\n",
       " -184.0,\n",
       " -188.0,\n",
       " -203.0,\n",
       " -120.0,\n",
       " -126.0,\n",
       " -116.0,\n",
       " -162.0,\n",
       " -137.0,\n",
       " -130.0,\n",
       " -119.0,\n",
       " -155.0,\n",
       " -115.0,\n",
       " -146.0,\n",
       " -110.0,\n",
       " -109.0,\n",
       " -138.0,\n",
       " -121.0,\n",
       " -177.0,\n",
       " -108.0,\n",
       " -150.0,\n",
       " -120.0,\n",
       " -130.0,\n",
       " -148.0,\n",
       " -167.0,\n",
       " -362.0,\n",
       " -175.0,\n",
       " -185.0,\n",
       " -136.0,\n",
       " -232.0,\n",
       " -186.0,\n",
       " -180.0,\n",
       " -113.0,\n",
       " -101.0,\n",
       " -125.0,\n",
       " -182.0,\n",
       " -128.0,\n",
       " -154.0,\n",
       " -114.0,\n",
       " -114.0,\n",
       " -109.0,\n",
       " -113.0,\n",
       " -164.0,\n",
       " -124.0,\n",
       " -133.0,\n",
       " -135.0,\n",
       " -154.0,\n",
       " -115.0,\n",
       " -171.0,\n",
       " -113.0,\n",
       " -131.0,\n",
       " -145.0,\n",
       " -170.0,\n",
       " -107.0,\n",
       " -164.0,\n",
       " -261.0,\n",
       " -134.0,\n",
       " -94.0,\n",
       " -119.0,\n",
       " -189.0,\n",
       " -144.0,\n",
       " -153.0,\n",
       " -141.0,\n",
       " -163.0,\n",
       " -175.0,\n",
       " -139.0,\n",
       " -136.0,\n",
       " -123.0,\n",
       " -162.0,\n",
       " -169.0,\n",
       " -180.0,\n",
       " -141.0,\n",
       " -213.0,\n",
       " -172.0,\n",
       " -114.0,\n",
       " -102.0,\n",
       " -140.0,\n",
       " -141.0,\n",
       " -138.0,\n",
       " -203.0,\n",
       " -167.0,\n",
       " -135.0,\n",
       " -144.0,\n",
       " -125.0,\n",
       " -121.0,\n",
       " -203.0,\n",
       " -137.0,\n",
       " -102.0,\n",
       " -269.0,\n",
       " -135.0,\n",
       " -250.0,\n",
       " -129.0,\n",
       " -142.0,\n",
       " -240.0,\n",
       " -382.0,\n",
       " -170.0,\n",
       " -123.0,\n",
       " -134.0,\n",
       " -168.0,\n",
       " -125.0,\n",
       " -133.0,\n",
       " -109.0,\n",
       " -139.0,\n",
       " -102.0,\n",
       " -168.0,\n",
       " -115.0,\n",
       " -120.0,\n",
       " -122.0,\n",
       " -109.0,\n",
       " -132.0,\n",
       " -157.0,\n",
       " -205.0,\n",
       " -171.0,\n",
       " -111.0,\n",
       " -139.0,\n",
       " -254.0,\n",
       " -167.0,\n",
       " -154.0,\n",
       " -151.0,\n",
       " -174.0,\n",
       " -138.0,\n",
       " -103.0,\n",
       " -116.0,\n",
       " -156.0,\n",
       " -120.0,\n",
       " -153.0,\n",
       " -104.0,\n",
       " -115.0,\n",
       " -121.0,\n",
       " -169.0,\n",
       " -181.0,\n",
       " -170.0,\n",
       " -98.0,\n",
       " -357.0,\n",
       " -150.0,\n",
       " -181.0,\n",
       " -129.0,\n",
       " -286.0,\n",
       " -91.0,\n",
       " -103.0,\n",
       " -159.0,\n",
       " -203.0,\n",
       " -146.0,\n",
       " -225.0,\n",
       " -111.0,\n",
       " -98.0,\n",
       " -120.0,\n",
       " -500.0,\n",
       " -329.0,\n",
       " -117.0,\n",
       " -145.0,\n",
       " -204.0,\n",
       " -120.0,\n",
       " -172.0,\n",
       " -108.0,\n",
       " -123.0,\n",
       " -500.0,\n",
       " -308.0,\n",
       " -164.0,\n",
       " -178.0,\n",
       " -240.0,\n",
       " -169.0,\n",
       " -332.0,\n",
       " -267.0,\n",
       " -258.0,\n",
       " -224.0,\n",
       " -131.0,\n",
       " -164.0,\n",
       " -227.0,\n",
       " -115.0,\n",
       " -307.0,\n",
       " -108.0,\n",
       " -500.0,\n",
       " -155.0,\n",
       " -256.0,\n",
       " -459.0,\n",
       " -105.0,\n",
       " -254.0,\n",
       " -208.0,\n",
       " -103.0,\n",
       " -170.0,\n",
       " -286.0,\n",
       " -177.0,\n",
       " -500.0,\n",
       " -127.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -434.0,\n",
       " -194.0,\n",
       " -299.0,\n",
       " -179.0,\n",
       " -467.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -275.0,\n",
       " -221.0,\n",
       " -225.0,\n",
       " -388.0,\n",
       " -353.0,\n",
       " -120.0,\n",
       " -215.0,\n",
       " -184.0,\n",
       " -185.0,\n",
       " -182.0,\n",
       " -500.0,\n",
       " -429.0,\n",
       " -205.0,\n",
       " -152.0,\n",
       " -174.0,\n",
       " -200.0,\n",
       " -129.0,\n",
       " -193.0,\n",
       " -262.0,\n",
       " -245.0,\n",
       " -142.0,\n",
       " -282.0,\n",
       " -235.0,\n",
       " -161.0,\n",
       " -175.0,\n",
       " -140.0,\n",
       " -158.0,\n",
       " -207.0,\n",
       " -195.0,\n",
       " -145.0,\n",
       " -162.0,\n",
       " -178.0,\n",
       " -150.0,\n",
       " -222.0,\n",
       " -141.0,\n",
       " -221.0,\n",
       " -165.0,\n",
       " -131.0,\n",
       " -195.0,\n",
       " -112.0,\n",
       " -170.0,\n",
       " -154.0,\n",
       " -171.0,\n",
       " -122.0,\n",
       " -126.0,\n",
       " -174.0,\n",
       " -191.0,\n",
       " -200.0,\n",
       " -185.0,\n",
       " -255.0,\n",
       " -403.0,\n",
       " -500.0,\n",
       " -193.0,\n",
       " -125.0,\n",
       " -178.0,\n",
       " -184.0,\n",
       " -154.0,\n",
       " -216.0,\n",
       " -431.0,\n",
       " -151.0,\n",
       " -354.0,\n",
       " -182.0,\n",
       " -232.0,\n",
       " -473.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -214.0,\n",
       " -141.0,\n",
       " -160.0,\n",
       " -210.0,\n",
       " -138.0,\n",
       " -167.0,\n",
       " -220.0,\n",
       " -478.0,\n",
       " -194.0,\n",
       " -500.0,\n",
       " -404.0,\n",
       " -264.0,\n",
       " -417.0,\n",
       " -220.0,\n",
       " -364.0,\n",
       " -353.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -462.0,\n",
       " -364.0,\n",
       " -275.0,\n",
       " -219.0,\n",
       " -120.0,\n",
       " -292.0,\n",
       " -152.0,\n",
       " -163.0,\n",
       " -210.0,\n",
       " -126.0,\n",
       " -127.0,\n",
       " -150.0,\n",
       " -190.0,\n",
       " -158.0,\n",
       " -170.0,\n",
       " -171.0,\n",
       " -211.0,\n",
       " -406.0,\n",
       " -185.0,\n",
       " -244.0,\n",
       " -219.0,\n",
       " -345.0,\n",
       " -500.0,\n",
       " -282.0,\n",
       " -235.0,\n",
       " -206.0,\n",
       " -234.0,\n",
       " -285.0,\n",
       " -261.0,\n",
       " -180.0,\n",
       " -410.0,\n",
       " -449.0,\n",
       " -500.0,\n",
       " -343.0,\n",
       " -202.0,\n",
       " -139.0,\n",
       " -319.0,\n",
       " -248.0,\n",
       " -222.0,\n",
       " -158.0,\n",
       " -204.0,\n",
       " -199.0,\n",
       " -240.0,\n",
       " -242.0,\n",
       " -173.0,\n",
       " -165.0,\n",
       " -136.0,\n",
       " -142.0,\n",
       " -176.0,\n",
       " -201.0,\n",
       " -253.0,\n",
       " -196.0,\n",
       " -282.0,\n",
       " -235.0,\n",
       " -222.0,\n",
       " -328.0,\n",
       " -326.0,\n",
       " -500.0,\n",
       " -218.0,\n",
       " -264.0,\n",
       " -324.0,\n",
       " -204.0,\n",
       " -360.0,\n",
       " -500.0,\n",
       " -458.0,\n",
       " -207.0,\n",
       " -368.0,\n",
       " -377.0,\n",
       " -289.0,\n",
       " -150.0,\n",
       " -288.0,\n",
       " -168.0,\n",
       " -182.0,\n",
       " -173.0,\n",
       " -277.0,\n",
       " -339.0,\n",
       " -422.0,\n",
       " -337.0,\n",
       " -200.0,\n",
       " -291.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -265.0,\n",
       " -169.0,\n",
       " -285.0,\n",
       " -500.0,\n",
       " -500.0,\n",
       " -308.0,\n",
       " -214.0,\n",
       " -356.0,\n",
       " -229.0,\n",
       " -187.0,\n",
       " -208.0,\n",
       " -353.0,\n",
       " -262.0,\n",
       " -215.0,\n",
       " -269.0,\n",
       " -235.0,\n",
       " -455.0,\n",
       " -145.0,\n",
       " -162.0,\n",
       " -254.0,\n",
       " -150.0,\n",
       " -166.0,\n",
       " -183.0,\n",
       " -204.0,\n",
       " -280.0,\n",
       " -215.0,\n",
       " -167.0,\n",
       " -170.0,\n",
       " -278.0,\n",
       " -268.0,\n",
       " -228.0,\n",
       " -208.0,\n",
       " -168.0,\n",
       " -500.0,\n",
       " -193.0,\n",
       " -500.0,\n",
       " -494.0,\n",
       " -219.0,\n",
       " -196.0,\n",
       " -373.0,\n",
       " -148.0,\n",
       " -152.0,\n",
       " -405.0,\n",
       " -213.0,\n",
       " -500.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
