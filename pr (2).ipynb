{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    import torch_directml\n",
    "    global device\n",
    "    device = torch_directml.device()\n",
    "except (NameError, ModuleNotFoundError):\n",
    "    import intel_npu_acceleration_library\n",
    "    from intel_npu_acceleration_library import compile\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "except (NameError, ModuleNotFoundError):\n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(id='ALE/SpaceInvaders-v5', entry_point='ale_py.env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, disable_env_checker=False, kwargs={'game': 'space_invaders', 'obs_type': 'grayscale', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 108000}, namespace='ALE', name='SpaceInvaders', version=5, additional_wrappers=(), vector_entry_point=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym  # As a best practice, Gymnasium is usually imported as 'gym'\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "import ale_py\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "env = gym.make('ALE/SpaceInvaders-v5', obs_type='grayscale')\n",
    "print(env.spec)\n",
    "\n",
    "plt.ion() # matplotlib를 interactive mode로 설정 -> 그래프를 실시간으로 업데이트할 수 있도록 함\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    \n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Reward')\n",
    "    \n",
    "    # 100 에피소드 이동 평균 계산 및 플롯\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100 Episode Avg', linestyle='--')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if is_ipython:\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # 매개변수 순서: 입력 데이터 채널 수(컬러면 3, 흑백이면 1), 출력 데이터 채널 수, ...)\n",
    "            nn.Conv2d(n_observations[0], 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # DuelingDQN 적용\n",
    "        self.fc_value = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(n_observations), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "            )\n",
    "        \n",
    "        self.fc_advantage = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(n_observations), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)  # 행동 이점은 행동 공간 크기\n",
    "        )\n",
    "\n",
    "    def feature_size(self, input_shape):\n",
    "        return self.conv(torch.zeros(1, *input_shape)).flatten().size(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / 255.0\n",
    "        x = self.conv(x).flatten(start_dim=1)\n",
    "\n",
    "        # 상태값과 행동 이점 계산\n",
    "        value = self.fc_value(x)\n",
    "        advantage = self.fc_advantage(x)\n",
    "\n",
    "        # 최종 Q값 계산\n",
    "        q_values = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class PrioritizedReplayMemory:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.priorities = deque([], maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def push(self, *args):\n",
    "        # 새로운 경험 추가\n",
    "        max_priority = max(self.priorities, default=1.0) # 가장 높은 우선순위로 초기화\n",
    "        self.memory.append(Transition(*args))\n",
    "        self.priorities.append(np.float32(max(1e-5, max_priority)))\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"우선순위 기반 샘플링\"\"\"\n",
    "        if len(self.memory) == 0:\n",
    "            raise ValueError(\"Memory is empty\")  # 메모리가 비어 있는 경우 예외 처리\n",
    "\n",
    "        priorities = np.array([float(p) for p in self.priorities], dtype=np.float32)  # 1차원 배열로 변환\n",
    "\n",
    "        if priorities.sum() == 0:\n",
    "            raise ValueError(\"All priorities are zero.\")  # 우선순위 합이 0인 경우 예외 처리\n",
    "\n",
    "        # 우선순위를 확률로 변환\n",
    "        probabilities = priorities ** self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        # 확률에 따라 샘플 선택\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
    "        samples = [self.memory[idx] for idx in indices]\n",
    "\n",
    "        # 중요도 가중치 계산\n",
    "        total = len(self.memory)\n",
    "        weights = (total * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()  # 가중치 정규화\n",
    "        weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "        return samples, weights, indices\n",
    "\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "            # 우선순위 업데이트\n",
    "            for idx, priority in zip(indices, priorities):\n",
    "                self.priorities[idx] = max(1e-5, priority) # 최소값 보장\n",
    "\n",
    "    def __len__(self):\n",
    "        # 저장된 메모리의 길이 반환\n",
    "        return len(self.memory)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    samples, weights, indices = memory.sample(BATCH_SIZE, beta=0.4)\n",
    "    batch = Transition(*zip(*samples))\n",
    "\n",
    "    # 다음 상태 중 None이 아닌 것만 선택\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "    # 모든 상태를 (배치 크기, 채널 수, 높이, 너비) 형식으로 연결\n",
    "    state_batch = torch.cat(batch.state).to(device)  # (BATCH_SIZE, 3, 210, 160)\n",
    "    action_batch = torch.cat(batch.action).to(device)\n",
    "    reward_batch = torch.cat(batch.reward).to(device) # 배치 처리를 최대한 병렬로 처리\n",
    "\n",
    "    # Q값 계산\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 다음 상태의 Q값 계산\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Double DQN으로 수정\n",
    "        next_actions = policy_net(non_final_next_states).max(1).indices.unsqueeze(1)\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "    # 기대 Q값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    td_errors = (state_action_values - expected_state_action_values.unsqueeze(1)).abs().detach()\n",
    "    td_errors = td_errors.cpu().numpy()\n",
    "    td_errors = np.maximum(td_errors, 1e-5)  # 최소값 보장\n",
    "    memory.update_priorities(indices, td_errors)\n",
    "\n",
    "    # Huber 손실 함수와 중요도 가중치 적용\n",
    "    criterion = nn.SmoothL1Loss(reduction='none')\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    loss = (weights * loss).mean() # 중요도 가중치 적용\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    state = state.to(device)\n",
    "\n",
    "    # 초기 eps_threshold는 1.0이다. steps_done이 증가함에 따라 점진적으로 감소하여 결국 Q-value가 가장 큰 행동을 선택.\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1,1)\n",
    "\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "resize = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((84, 84)),  # 크기 조정\n",
    "    T.ToTensor()         # 텐서 변환\n",
    "])\n",
    "\n",
    "def preprocess_state(state):\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    state = resize(state)\n",
    "    state = state.unsqueeze(0)  # 차원 추가 grayscale (84, 84) -> (1, 1, 84, 84)으로 변경\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards, path=\"models\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    save_path = os.path.join(path, f\"policy_net_episode_{episode}.pth\")\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model_state_dict': policy_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'steps_done': steps_done,  # 탐험 상태 저장\n",
    "        'memory': list(memory.memory), # 경험 저장\n",
    "        'priorities': list(memory.priorities), # 우선순위 저장\n",
    "        'episode_rewards': episode_rewards\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def load_model(policy_net, optimizer, path):\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    # 모델 가중치와 옵티마이저 복원\n",
    "    policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # 에피소드 및 탐험상태 복원\n",
    "    episode = checkpoint['episode']\n",
    "    steps_done = checkpoint.get('steps_done', 0)  # 기본값 0\n",
    "\n",
    "    # 리플레이 메모리복원\n",
    "    memory_data = checkpoint.get('memory', [])\n",
    "    priority_data = checkpoint.get('priorities', [])\n",
    "    if memory_data and priority_data:\n",
    "        memory.memory = deque(memory_data, maxlen=memory.capacity)\n",
    "        memory.priorities = deque(priority_data, maxlen=memory.capacity)\n",
    "    \n",
    "    # 에피소드별 보상 복원\n",
    "    episode_rewards = checkpoint.get('episode_rewards', [])\n",
    "\n",
    "    print(f\"Model loaded from {path}, starting from episode {episode}, steps_done={steps_done}\")\n",
    "    return episode, steps_done, episode_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 10000 # 크게 할수록 캄험이 더 오래 지속 -> 새로운 더 많이 탐색\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "memory_capacity = 50000 # 크게 할수록 에이전트가 더 많은 경험을 저장\n",
    "\n",
    "n_actions = env.action_space.n # 18\n",
    "\n",
    "n_observations = (4, 84, 84)\n",
    "\n",
    "# 입력 텐서 형식은 (배치 크기, 채널 수, 높이, 너비)\n",
    "policy_net = DuelingDQN(n_observations, n_actions).to(device)\n",
    "target_net = DuelingDQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = PrioritizedReplayMemory(memory_capacity) # 10000\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/policy_net_episode_4.pth, starting from episode 4, steps_done=0\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델에서 학습 재개\n",
    "checkpoint_path = \"models/policy_net_episode_4.pth\"\n",
    "start_episode, steps_done, episode_rewards = load_model(policy_net, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted. Saving the current model...\n",
      "Model saved to models/policy_net_episode_5.pth\n",
      "Model saved. Exiting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    for episode in range(num_episodes+1):\n",
    "        # if episode % 100 == 0 and episode != 0:\n",
    "        #     env = gym.make('ALE/Breakout-v5', obs_type='grayscale', render_mode='human')\n",
    "        # else:\n",
    "        #     env = gym.make('ALE/Breakout-v5', obs_type='grayscale')\n",
    "            \n",
    "        state, info = env.reset()\n",
    "        state_stack = deque([preprocess_state(state)] * 4,maxlen=4)\n",
    "        state = torch.cat(list(state_stack), dim=1)\n",
    "\n",
    "        episode_reward = 0\n",
    "        action_counts = np.zeros(env.action_space.n)    \n",
    "\n",
    "        for t in count(): # 무한 반복\n",
    "            if episode < 10:  # 초기 10 에피소드는 무조건 랜덤 행동\n",
    "                action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "            else:\n",
    "                action = select_action(state)\n",
    "\n",
    "            action_counts[action] += 1\n",
    "\n",
    "            # 행동을 선택하면, 해당 행동이 4개의 연속된 프레임 동안 환경에 반복적으로 적용\n",
    "            # 프레임 스킵 동안 발생한 보상이 모두 합산되어 반환, 마지막 프레임의 상태와 종료 여부가 반환\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward * 10], device=device)\n",
    "            episode_reward += reward.item()\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if not done:\n",
    "                next_state = preprocess_state(next_state)\n",
    "                state_stack.append(next_state)\n",
    "                next_state = torch.cat(list(state_stack), dim=1)\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # policy network 학습 \n",
    "            if t % 4 == 0:\n",
    "                optimize_model()\n",
    "\n",
    "            if t % 8 == 0:\n",
    "                # 소프트 업데이트 적용 (target network 업데이트) -> 16 프레임마다 학습이 이루어짐\n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                break\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards)\n",
    "            \n",
    "        if episode % 5 == 0:\n",
    "            # 행동 비율 출력\n",
    "            action_distribution = action_counts / action_counts.sum()\n",
    "            print(f\"Action distribution: {action_distribution}\")            \n",
    "            plot_rewards()\n",
    "\n",
    "    print('Complete')\n",
    "    plot_rewards(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving the current model...\")\n",
    "    save_model(episode, policy_net, optimizer, steps_done, memory, episode_rewards)\n",
    "    env.close()\n",
    "    print(\"Model saved. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
