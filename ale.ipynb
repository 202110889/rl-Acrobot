{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "gym.register_envs(ale_py)\n",
    "from IPython import display\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "import torch\n",
    "import torch_directml\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from torchvision.transforms import ToTensor\n",
    "import math\n",
    "import numpy as np\n",
    "try:\n",
    "    global device\n",
    "    device = torch_directml.device()\n",
    "except NameError:\n",
    "    device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else\n",
    "            \"mps\" if torch.backends.mps.is_available() else\n",
    "            \"cpu\"\n",
    "        )\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "base_env = gym.make('ALE/IceHockey-v5', render_mode='rgb_array')\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", disable_logger=True)\n",
    "done = False\n",
    "\n",
    "obs, info = env.reset()\n",
    "t = 0\n",
    "max_steps = 200\n",
    "\n",
    "# Simulate an episode\n",
    "while not done:\n",
    "\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    done = terminated or truncated or t > max_steps\n",
    "    t += 1\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "\n",
    "# Render recording\n",
    "widgets.Video.from_file(\n",
    "    f\"./videos/rl-video-episode-0.mp4\", autoplay=False, loop=False, width=700\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obs: array(210, 160, 3)\n",
    "    + obs dataset: array(n, 210, 160, 3)\n",
    "- reward: float\n",
    "\n",
    "설계: 에피소드를 플레이해 기억 버퍼에 저장, 4개씩 임의로 뽑아 상태로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
    "# Transition이란 이름을 일종의 구조체로\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, n_actions)\n",
    "            )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5),  # (42, 32)\n",
    "            nn.Conv2d(192, 768, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (21, 16)\n",
    "            nn.Conv2d(768, 64, 3, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        conv0 = self.conv(x)\n",
    "        conv0_result = self.fc(conv0)\n",
    "        return conv0_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = state.shape\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "# 처음에는 파라미터가 완전히 같게 시작\n",
    "# 코딩 실습: 강화학습 과제 시간에 하기\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([4, 3, 210, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1966, -0.1492,  2.5425, -0.4242,  2.1744,  0.7381,  0.3016, -4.8014,\n",
       "         -3.8713,  6.2697, -2.4546, -0.2726,  4.1840,  2.4821, -0.7777, -6.5005,\n",
       "          0.8856,  3.9952],\n",
       "        [ 0.1966, -0.1492,  2.5425, -0.4242,  2.1744,  0.7381,  0.3016, -4.8014,\n",
       "         -3.8713,  6.2697, -2.4546, -0.2726,  4.1840,  2.4821, -0.7777, -6.5005,\n",
       "          0.8856,  3.9952],\n",
       "        [ 0.1966, -0.1492,  2.5425, -0.4242,  2.1744,  0.7381,  0.3016, -4.8014,\n",
       "         -3.8713,  6.2697, -2.4546, -0.2726,  4.1840,  2.4821, -0.7777, -6.5005,\n",
       "          0.8856,  3.9952],\n",
       "        [ 0.1966, -0.1492,  2.5425, -0.4242,  2.1744,  0.7381,  0.3016, -4.8014,\n",
       "         -3.8713,  6.2697, -2.4546, -0.2726,  4.1840,  2.4821, -0.7777, -6.5005,\n",
       "          0.8856,  3.9952]], device='privateuseone:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module test\n",
    "\n",
    "test_state = []\n",
    "for _ in range(4):\n",
    "    test_state.append(env.step(0)[0])\n",
    "test_state = np.array(test_state, dtype=np.float64) # (4, 210, 160, 3)\n",
    "test_state = test_state.transpose(0, 3, 1, 2) # (4, 3, 210, 160)\n",
    "test_state = torch.Tensor(test_state).to(device=device)\n",
    "print(test_state.dtype)\n",
    "policy_net(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
